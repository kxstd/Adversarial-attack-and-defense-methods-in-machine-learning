{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "209745f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms as tt\n",
    "import torchvision.models as models\n",
    "import timm\n",
    "from PIL import Image as pil\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "vit = timm.create_model('vit_small_patch16_224', pretrained=True)\n",
    "image_path=\"images\"\n",
    "old_label_path=\"images//old_labels\"\n",
    "image_list=os.listdir(image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1da5b6",
   "metadata": {},
   "source": [
    "## 图片导入与数据集生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "bbd2423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5]):\n",
    "    mean_t = torch.Tensor(mean).reshape([1,3,1,1]).to(x.device)\n",
    "    std_t = torch.Tensor(std).reshape([1,3,1,1]).to(x.device)\n",
    "    y = (x-mean_t)/std_t\n",
    "    return y\n",
    "\n",
    "def get_preprocess(model_name):\n",
    "    if model_name[:6]==\"resnet\":\n",
    "        mean=[0.485, 0.456, 0.406]\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    elif model_name[:3]==\"vit\":\n",
    "        mean=[0.5,0.5,0.5]\n",
    "        std=[0.5,0.5,0.5]\n",
    "    def preprocess(images):\n",
    "        y = F.interpolate(images.unsqueeze(0),224)\n",
    "        return normalize(y, mean=mean,\n",
    "                                 std=std)\n",
    "    return preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "3b7c171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mydata1(Dataset):\n",
    "    def __init__(self,image_path):\n",
    "        self.file=[]\n",
    "        for i in open(old_label_path):\n",
    "            self.file.append(i)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img=pil.open(\"images//\"+self.file[idx].split(' ')[0])\n",
    "        label=np.array(int(self.file[idx].split(' ')[1]))\n",
    "        img=preprocess1(tt.ToTensor()(img))\n",
    "        return img[0], label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file)\n",
    "    \n",
    "    \n",
    "class mydata2(Dataset):\n",
    "    def __init__(self,image_path):\n",
    "        self.file=[]\n",
    "        for i in open(old_label_path):\n",
    "            self.file.append(i)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img=pil.open(\"images//\"+self.file[idx].split(' ')[0])\n",
    "        label=np.array(int(self.file[idx].split(' ')[1]))\n",
    "        img=preprocess2(tt.ToTensor()(img))\n",
    "        return img[0], label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file)\n",
    "preprocess1=get_preprocess(\"resnet\")\n",
    "res_data_Tensor = torch.utils.data.DataLoader(dataset=mydata1(image_path))\n",
    "preprocess2=get_preprocess(\"vit\")\n",
    "vit_data_Tensor=torch.utils.data.DataLoader(dataset=mydata2(image_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc3d0b1",
   "metadata": {},
   "source": [
    "##  测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "289a1a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,test_Tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_Tensor:\n",
    "            test_output = model(images)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            correct=correct+(pred_y == labels)\n",
    "            total=total+1\n",
    "        accuracy = correct / float(total)\n",
    "            \n",
    "    print('Test Accuracy of the model on the 1000 test images: %.3f' % accuracy)\n",
    "    print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1165ae8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 1000 test images: 0.950\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "test(resnet50,res_data_Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "da017454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 1000 test images: 0.978\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "test(vit,vit_data_Tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca442984",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0388b5",
   "metadata": {},
   "source": [
    "## 对抗样本生成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6916a885",
   "metadata": {},
   "source": [
    "### FGSM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99933bd",
   "metadata": {},
   "source": [
    "#### L_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "64b9da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon =1/255.0 #最大扰动\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "def fgsm(input,epsilon,data_grad):\n",
    "    grad_attack = data_grad.sign()\n",
    "    result = input + epsilon*grad_attack\n",
    "    result = torch.clamp(result, 0, 1)\n",
    "    return result\n",
    "\n",
    "def fgsm_attack(model,input_Tensor,epsilon):\n",
    "    fgsm_sample_list=[]\n",
    "    id=0\n",
    "    for (data, target) in input_Tensor:\n",
    "        data.requires_grad = True\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1)[1].squeeze()\n",
    "        loss = loss_fn(output, target.long())\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        data_grad = data.grad.data\n",
    "        perturbed_data = fgsm(data,epsilon,data_grad)\n",
    "        fgsm_sample_list.append((perturbed_data,target))\n",
    "        id=id+1\n",
    "        if(id%100==0):\n",
    "            print((\"%d/\"+\"1000\"+\"\\r\")%(id))\n",
    "        \n",
    "    return fgsm_sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "deac012c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/1000\n",
      "200/1000\n",
      "300/1000\n",
      "400/1000\n",
      "500/1000\n",
      "600/1000\n",
      "700/1000\n",
      "800/1000\n",
      "900/1000\n",
      "1000/1000\n"
     ]
    }
   ],
   "source": [
    "fgsm_sample={}\n",
    "fgsm_sample_path='./fgsm_sample.txt'\n",
    "fgsm_sample[\"res\"]=fgsm_attack(resnet50, res_data_Tensor,epsilon)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e9a61549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/1000\n",
      "200/1000\n",
      "300/1000\n",
      "400/1000\n",
      "500/1000\n",
      "600/1000\n",
      "700/1000\n",
      "800/1000\n",
      "900/1000\n",
      "1000/1000\n"
     ]
    }
   ],
   "source": [
    "fgsm_sample[\"vit\"]=fgsm_attack(vit, vit_data_Tensor,epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "55c3711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fgsm_sample_path,'wb') as f:\n",
    "    content = pickle.dumps(fgsm_sample)\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d6ffb878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 1000 test images: 0.379\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "test(resnet50,fgsm_sample[\"res\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "254731ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 1000 test images: 0.739\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "test(vit,fgsm_sample[\"vit\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68f9382",
   "metadata": {},
   "source": [
    "### PGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d0ae2",
   "metadata": {},
   "source": [
    "#### L_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a8dce47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 6     #迭代次数\n",
    "alpha =epsilon*2/iters    #迭代步长\n",
    "\n",
    "def PGD(input,epsilon,data_grad,iters,alpha):\n",
    "    grad_attack = data_grad.sign()\n",
    "    delta = grad_attack*alpha\n",
    "    result = input+delta\n",
    "    result = torch.clamp(result,0,1)\n",
    "    return result\n",
    "        \n",
    "  \n",
    "\n",
    "def PGD_attack(model,input_Tensor,epsilon,iters,alpha):\n",
    "    pgd_sample_list=[]\n",
    "    for (data,target) in input_Tensor:\n",
    "        perturbed_data=data.detach()\n",
    "        for i in range(iters):\n",
    "            perturbed_data.requires_grad = True\n",
    "            output = model(perturbed_data)\n",
    "            init_pred = output.max(1)[1].squeeze()\n",
    "            loss = loss_fn(output, target.long())\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            perturbed_data_grad = perturbed_data.grad.data\n",
    "            disturb = torch.clamp(PGD(perturbed_data,epsilon,perturbed_data_grad,iters,alpha)-data,-epsilon,epsilon)\n",
    "            perturbed_data=(data+disturb).detach()\n",
    "        pgd_sample_list.append((perturbed_data,target)) \n",
    "        \n",
    "        \n",
    "    return pgd_sample_list\n",
    "\n",
    "pgd_sample={}\n",
    "pgd_sample_path='./pgd_sample.txt'\n",
    "\n",
    "\n",
    "pgd_sample[\"vit\"]=PGD_attack(vit,vit_data_Tensor,epsilon,iters,alpha)\n",
    "pgd_sample[\"res\"]=PGD_attack(resnet50,res_data_Tensor,epsilon,iters,alpha)\n",
    "    \n",
    "with open(pgd_sample_path,'wb') as f:\n",
    "    content = pickle.dumps(pgd_sample)\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b4329fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 1000 test images: 0.628\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "test(resnet50,pgd_sample[\"res\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "aa4e694d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 1000 test images: 0.741\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "test(vit,pgd_sample[\"vit\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce9325c",
   "metadata": {},
   "source": [
    "#### L_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5eaf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 6     #迭代次数\n",
    "alpha =epsilon*2/iters    #迭代步长\n",
    "\n",
    "def PGD(input,epsilon,data_grad,iters,alpha):\n",
    "    grad_attack = data_grad.sign()\n",
    "    delta = grad_attack*alpha \n",
    "    result = input+delta\n",
    "    result = torch.clamp(result,0,1)\n",
    "    return result\n",
    "        \n",
    "  \n",
    "\n",
    "def PGD_attack(model,input_Tensor,epsilon,iters,alpha):\n",
    "    pgd_sample_list=[]\n",
    "    for (data,target) in input_Tensor:\n",
    "        perturbed_data=data.detach()\n",
    "        for i in range(iters):\n",
    "            perturbed_data.requires_grad = True\n",
    "            output = model(perturbed_data)\n",
    "            init_pred = output.max(1)[1].squeeze()\n",
    "            loss = loss_fn(output, target.long())\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            perturbed_data_grad = perturbed_data.grad.data\n",
    "            disturb = torch.clamp(PGD(perturbed_data,epsilon,perturbed_data_grad,iters,alpha)-data,-epsilon,epsilon)\n",
    "            perturbed_data=(data+disturb).detach()\n",
    "        pgd_sample_list.append((perturbed_data,target)) \n",
    "        \n",
    "        \n",
    "    return pgd_sample_list\n",
    "\n",
    "pgd_sample={}\n",
    "pgd_sample_path='./pgd_sample.txt'\n",
    "\n",
    "\n",
    "pgd_sample[\"vit\"]=PGD_attack(vit,vit_data_Tensor,epsilon,iters,alpha)\n",
    "pgd_sample[\"res\"]=PGD_attack(resnet50,res_data_Tensor,epsilon,iters,alpha)\n",
    "    \n",
    "with open(pgd_sample_path,'wb') as f:\n",
    "    content = pickle.dumps(pgd_sample)\n",
    "    f.write(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
