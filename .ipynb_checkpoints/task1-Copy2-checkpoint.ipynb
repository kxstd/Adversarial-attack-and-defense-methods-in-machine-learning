{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "209745f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms as tt\n",
    "import torchvision.models as models\n",
    "import timm\n",
    "from PIL import Image as pil\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from numpy import linalg as LA\n",
    "\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "vit = timm.create_model('vit_small_patch16_224', pretrained=True)\n",
    "image_path=\"images\"\n",
    "old_label_path=\"images//old_labels\"\n",
    "image_list=os.listdir(image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1da5b6",
   "metadata": {},
   "source": [
    "## 图片导入与数据集生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbd2423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5]):\n",
    "    mean_t = torch.Tensor(mean).reshape([1,3,1,1]).to(x.device)\n",
    "    std_t = torch.Tensor(std).reshape([1,3,1,1]).to(x.device)\n",
    "    y = (x-mean_t)/std_t\n",
    "    return y\n",
    "\n",
    "def get_preprocess(model_name):\n",
    "    if model_name[:6]==\"resnet\":\n",
    "        mean=[0.485, 0.456, 0.406]\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    elif model_name[:3]==\"vit\":\n",
    "        mean=[0.5,0.5,0.5]\n",
    "        std=[0.5,0.5,0.5]\n",
    "    def preprocess(images):\n",
    "        y = F.interpolate(images.unsqueeze(0),224)\n",
    "        return normalize(y, mean=mean,\n",
    "                                 std=std)\n",
    "    return preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b7c171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mydata1(Dataset):\n",
    "    def __init__(self,image_path):\n",
    "        self.file=[]\n",
    "        for i in open(old_label_path):\n",
    "            self.file.append(i)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img=pil.open(\"images//\"+self.file[idx].split(' ')[0])\n",
    "        label=np.array(int(self.file[idx].split(' ')[1]))\n",
    "        img=preprocess1(tt.ToTensor()(img))\n",
    "        return img[0], label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file)\n",
    "    \n",
    "    \n",
    "class mydata2(Dataset):\n",
    "    def __init__(self,image_path):\n",
    "        self.file=[]\n",
    "        for i in open(old_label_path):\n",
    "            self.file.append(i)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img=pil.open(\"images//\"+self.file[idx].split(' ')[0])\n",
    "        label=np.array(int(self.file[idx].split(' ')[1]))\n",
    "        img=preprocess2(tt.ToTensor()(img))\n",
    "        return img[0], label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file)\n",
    "preprocess1=get_preprocess(\"resnet\")\n",
    "res_data_Tensor = torch.utils.data.DataLoader(dataset=mydata1(image_path))\n",
    "preprocess2=get_preprocess(\"vit\")\n",
    "vit_data_Tensor=torch.utils.data.DataLoader(dataset=mydata2(image_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc3d0b1",
   "metadata": {},
   "source": [
    "##  测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "289a1a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,test_Tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_Tensor:\n",
    "            test_output = model(images)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            correct=correct+(pred_y == labels)\n",
    "            total=total+1\n",
    "        accuracy = correct / float(total)\n",
    "            \n",
    "    print('Test Accuracy of the model on the 1000 test images: %.3f' % accuracy)\n",
    "    print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1165ae8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-873e33b53c85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mres_data_Tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-07d254596dc6>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(model, test_Tensor)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_Tensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[0mtest_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mpred_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-8ac9c83d76c9>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mimg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"images//\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mimg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpreprocess1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \"\"\"\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m255\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteStorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetbands\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mtobytes\u001b[1;34m(self, encoder_name, *args)\u001b[0m\n\u001b[0;32m    732\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 734\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m         \u001b[1;31m# unpack data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda\\lib\\site-packages\\PIL\\ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m                             \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m                                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test(resnet50,res_data_Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da017454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test(vit,vit_data_Tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca442984",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0388b5",
   "metadata": {},
   "source": [
    "## 对抗样本生成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebd1264",
   "metadata": {},
   "source": [
    "### C&W attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb2913a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss1_func(w,x,d,c):\n",
    "    return torch.dist(x,(torch.tanh(w)*d+c),p=2)\n",
    "\n",
    "def f(output,tlab,target,k=0):\n",
    "    #f6(x)=max((max(Z(x')i)-Z(x')t),-k)\n",
    "    real= torch.max(output*tlab)\n",
    "    second=torch.max((1-tlab)*output)\n",
    "    #如果指定了对象，则让这个更接近，否则选择第二个较大的\n",
    "    if(target):\n",
    "        return torch.max(second - real, torch.Tensor([-k]).cuda())\n",
    "    else:\n",
    "        return torch.max(real - second, torch.Tensor([-k]).cuda())\n",
    "\n",
    "def cwattack_l2(img,model,right_label,iteration=1000,lr=0.001,target=False,target_label=0):\n",
    "    shape=(1,3,28,28)\n",
    "    binary_number=9                         #二分搜索\n",
    "    maxc=1e10                               #从0.01-100去找c\n",
    "    minc=0\n",
    "    c=1e-3                                   #from c = 0:01 to c = 100 on the MNIST dataset.\n",
    "    min_loss=1000000                         #找到最小的loss，即为方程的解\n",
    "    min_loss_img=img                         #扰动后的图片\n",
    "    k=0                                      #f函数使用，论文默认为0\n",
    "    b_min = 0                                #盒约束，论文中使用了0-1 代码中-0.5 0.5\n",
    "    b_max = 1\n",
    "    b_mul=(b_max-b_min)/2.0\n",
    "    b_plus=(b_min+b_max)/2.0\n",
    "    if(not target):\n",
    "        target_label=right_label\n",
    "    tlab=Variable(torch.from_numpy(np.eye(10)[target_label]).cuda().float())\n",
    "    for binary_index in range(binary_number):\n",
    "        print(\"------------Start {} search, current c is {}------------\".format(binary_index,c))\n",
    "        \n",
    "        #将img转换为w，w=arctanh(2x-1)，作为原始图片\n",
    "        w = Variable(torch.from_numpy(np.arctanh((img.numpy()-b_plus)/b_mul*0.99999)).float()).cuda()\n",
    "        \n",
    "        w_pert=Variable(torch.zeros_like(w).cuda().float())\n",
    "        w_pert.requires_grad = True\n",
    "        #最初图像x\n",
    "        x= Variable(img).cuda()\n",
    "        optimizer = optim.Adam([w_pert], lr=lr) #论文p3选用Adam  \n",
    "        isSuccessfulAttack=False\n",
    "        \n",
    "        for iteration_index in range(1,iteration+1):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # w加入扰动w_pert之后的新图像\n",
    "            img_new=torch.tanh(w_pert+w)*b_mul+b_plus  #0.5*tanh(w+pert)+0.5\n",
    "            loss_1=loss1_func(w,img_new,b_mul,b_plus)  #\\\\ deta\\\\  p=2\n",
    "            model.eval()\n",
    "            output=model(img_new)                      # Z(x)\n",
    "            loss_2=c*f(output,tlab,target)             # c*f(x+deta) , x+deta=img_new, \n",
    "            loss=loss_1+loss_2                         # Minimize loss=loss1+loss2\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            if iteration_index%200==0:\n",
    "                 print('Iters: [{}/{}]\\tLoss: {},Loss1(L2 distance):{}, Loss2:{}'\n",
    "                       .format(iteration_index,iteration,loss.item(),loss_1.item(),loss_2.item()))\n",
    "                    \n",
    "            pred_result=output.argmax(1, keepdim=True).item()\n",
    "            #指定目标模式下,此处考虑l2距离最小,即找到最小的loss1\n",
    "            if(target):\n",
    "                if(min_loss>loss_1 and pred_result==target_label):\n",
    "                    flag=False\n",
    "                    for i in range(20):\n",
    "                        output=model(img_new)\n",
    "                        pred_result=output.argmax(1, keepdim=True).item()\n",
    "                        if(pred_result!=target_label):\n",
    "                            flag=True  #原模型中存在dropout，此处判断连续成功攻击20次，则视为有效\n",
    "                            break\n",
    "                    if(flag):\n",
    "                        continue\n",
    "                    min_loss=loss_1\n",
    "                    min_loss_img=img_new\n",
    "                    print('success when loss: {}, pred: {}'.format(min_loss,pred_result))\n",
    "                    isSuccessfulAttack=True\n",
    "             #非目标模式，找到最接近的一个,连续20次不预测成功\n",
    "            else:\n",
    "                if(min_loss>loss_1 and pred_result!=right_label):\n",
    "                    flag=False\n",
    "                    for i in range(50):\n",
    "                        output=model(img_new)\n",
    "                        pred_result=output.argmax(1, keepdim=True).item()\n",
    "                        if(pred_result==right_label):\n",
    "                            flag=True  #原模型中存在dropout，此处判断连续成功攻击50次，则视为有效\n",
    "                            break\n",
    "                    if(flag):\n",
    "                        continue\n",
    "                    min_loss=loss_1\n",
    "                    min_loss_img=img_new\n",
    "                    print('success when loss: {}, pred: {}'.format(min_loss,pred_result))\n",
    "                    isSuccessfulAttack=True\n",
    "        if(isSuccessfulAttack):\n",
    "            maxc=min(maxc,c)\n",
    "            if maxc<1e9: \n",
    "                c=(minc+maxc)/2\n",
    "        #攻击失败，尝试放大c\n",
    "        else:\n",
    "            minc=max(minc,c)\n",
    "            if(maxc<1e9):\n",
    "                c=(maxc+minc)/2\n",
    "            else:\n",
    "                c=c*10\n",
    "    return min_loss_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77d481ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def cw_attack(model,input_Tensor,epsilon,lim):\n",
    "    cw_sample_list=[]\n",
    "    id=0\n",
    "    for (data, target) in input_Tensor:\n",
    "        print(\"Correct Label is \",target)\n",
    "\n",
    "        victim_img_input2 = torch.unsqueeze(data, 0)\n",
    "        attack_untarget_img = cwattack_l2(data,resnet50,target,\n",
    "                         iteration=1000,lr=0.01)\n",
    "        cw_sample_list.append(attack_untarget_img)\n",
    "        id=id+1\n",
    "        if(id%100==0):\n",
    "            print((\"%d/\"+\"1000\"+\"\\r\")%(id))\n",
    "        \n",
    "    return fgsm_sample_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6916a885",
   "metadata": {},
   "source": [
    "### FGSM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99933bd",
   "metadata": {},
   "source": [
    "#### L_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64b9da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def fgm(model, image, label, epsilon, order, clip_min, clip_max, device):\n",
    "    imageArray = image.cpu().detach().numpy()\n",
    "    X_fgsm = torch.tensor(imageArray).to(device)\n",
    "\n",
    "    #print(image.data)\n",
    "\n",
    "    X_fgsm.requires_grad = True\n",
    "\n",
    "    opt = optim.SGD([X_fgsm], lr=1e-3)\n",
    "    opt.zero_grad()\n",
    "\n",
    "    loss = nn.CrossEntropyLoss()(model(X_fgsm), label.long())\n",
    "\n",
    "    loss.backward()\n",
    "    #print(X_fgsm)\n",
    "    #print(X_fgsm.grad)\n",
    "    if order == np.inf:\n",
    "        d = epsilon * X_fgsm.grad.data.sign()\n",
    "    elif order == 2:\n",
    "        gradient = X_fgsm.grad\n",
    "        d = torch.zeros(gradient.shape, device = device)\n",
    "        for i in range(gradient.shape[0]):\n",
    "            norm_grad = gradient[i].data/LA.norm(gradient[i].data.cpu().numpy())\n",
    "            d[i] = norm_grad * epsilon\n",
    "    else:\n",
    "        raise ValueError('Other p norms may need other algorithms')\n",
    "\n",
    "    x_adv = X_fgsm + d\n",
    "\n",
    "    if clip_max == None and clip_min == None:\n",
    "        clip_max = np.inf\n",
    "        clip_min = -np.inf\n",
    "\n",
    "    x_adv = torch.clamp(x_adv,clip_min, clip_max)\n",
    "\n",
    "    return x_adv\n",
    "        \n",
    "\n",
    "def fgsm_attack(model,input_Tensor,epsilon,lim):\n",
    "    fgsm_sample_list=[]\n",
    "    id=0\n",
    "    for (data, target) in input_Tensor:\n",
    "        perturbed_data = fgm(model,data,target,epsilon,2,0,1,\"cpu\")\n",
    "        fgsm_sample_list.append((perturbed_data,target))\n",
    "        id=id+1\n",
    "        if(id%100==0):\n",
    "            print((\"%d/\"+\"1000\"+\"\\r\")%(id))\n",
    "        \n",
    "    return fgsm_sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "deac012c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/1000\n",
      "200/1000\n",
      "300/1000\n",
      "400/1000\n",
      "500/1000\n",
      "600/1000\n",
      "700/1000\n",
      "800/1000\n",
      "900/1000\n",
      "1000/1000\n"
     ]
    }
   ],
   "source": [
    "epsilon=0.3\n",
    "fgsm_sample={}\n",
    "fgsm_sample_path='./fgsm_sample.txt'\n",
    "fgsm_sample[\"res\"]=fgsm_attack(resnet50, res_data_Tensor,epsilon,lim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a61549",
   "metadata": {},
   "outputs": [],
   "source": [
    "fgsm_sample[\"vit\"]=fgsm_attack(vit, vit_data_Tensor,epsilon,lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c3711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fgsm_sample_path,'wb') as f:\n",
    "    content = pickle.dumps(fgsm_sample)\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6ffb878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 1000 test images: 0.181\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "test(resnet50,fgsm_sample[\"res\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254731ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(vit,fgsm_sample[\"vit\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1466ff47",
   "metadata": {},
   "source": [
    "#### L_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42fd3fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d05819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d68f9382",
   "metadata": {},
   "source": [
    "### PGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d0ae2",
   "metadata": {},
   "source": [
    "#### L_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dce47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 6     #迭代次数\n",
    "alpha =epsilon*2/iters    #迭代步长\n",
    "\n",
    "def PGD(input,epsilon,data_grad,iters,alpha):\n",
    "    grad_attack = data_grad.sign()\n",
    "    delta = grad_attack*alpha\n",
    "    result = input+delta\n",
    "    result = torch.clamp(result,0,1)\n",
    "    return result\n",
    "        \n",
    "  \n",
    "\n",
    "def PGD_attack(model,input_Tensor,epsilon,iters,alpha):\n",
    "    pgd_sample_list=[]\n",
    "    for (data,target) in input_Tensor:\n",
    "        perturbed_data=data.detach()\n",
    "        for i in range(iters):\n",
    "            perturbed_data.requires_grad = True\n",
    "            output = model(perturbed_data)\n",
    "            init_pred = output.max(1)[1].squeeze()\n",
    "            loss = loss_fn(output, target.long())\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            perturbed_data_grad = perturbed_data.grad.data\n",
    "            disturb = torch.clamp(PGD(perturbed_data,epsilon,perturbed_data_grad,iters,alpha)-data,-epsilon,epsilon)\n",
    "            perturbed_data=(data+disturb).detach()\n",
    "        pgd_sample_list.append((perturbed_data,target)) \n",
    "        \n",
    "        \n",
    "    return pgd_sample_list\n",
    "\n",
    "pgd_sample={}\n",
    "pgd_sample_path='./pgd_sample.txt'\n",
    "\n",
    "\n",
    "pgd_sample[\"vit\"]=PGD_attack(vit,vit_data_Tensor,epsilon,iters,alpha)\n",
    "pgd_sample[\"res\"]=PGD_attack(resnet50,res_data_Tensor,epsilon,iters,alpha)\n",
    "    \n",
    "with open(pgd_sample_path,'wb') as f:\n",
    "    content = pickle.dumps(pgd_sample)\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4329fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(resnet50,pgd_sample[\"res\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4e694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(vit,pgd_sample[\"vit\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce9325c",
   "metadata": {},
   "source": [
    "#### L_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5eaf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 6     #迭代次数\n",
    "alpha =epsilon*2/iters    #迭代步长\n",
    "\n",
    "def PGD(input,epsilon,data_grad,iters,alpha):\n",
    "    grad_attack = data_grad.sign()\n",
    "    delta = grad_attack*alpha \n",
    "    result = input+delta\n",
    "    result = torch.clamp(result,0,1)\n",
    "    return result\n",
    "        \n",
    "  \n",
    "\n",
    "def PGD_attack(model,input_Tensor,epsilon,iters,alpha):\n",
    "    pgd_sample_list=[]\n",
    "    for (data,target) in input_Tensor:\n",
    "        perturbed_data=data.detach()\n",
    "        for i in range(iters):\n",
    "            perturbed_data.requires_grad = True\n",
    "            output = model(perturbed_data)\n",
    "            init_pred = output.max(1)[1].squeeze()\n",
    "            loss = loss_fn(output, target.long())\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            perturbed_data_grad = perturbed_data.grad.data\n",
    "            disturb = torch.clamp(PGD(perturbed_data,epsilon,perturbed_data_grad,iters,alpha)-data,-epsilon,epsilon)\n",
    "            perturbed_data=(data+disturb).detach()\n",
    "        pgd_sample_list.append((perturbed_data,target)) \n",
    "        \n",
    "        \n",
    "    return pgd_sample_list\n",
    "\n",
    "pgd_sample={}\n",
    "pgd_sample_path='./pgd_sample.txt'\n",
    "\n",
    "\n",
    "pgd_sample[\"vit\"]=PGD_attack(vit,vit_data_Tensor,epsilon,iters,alpha)\n",
    "pgd_sample[\"res\"]=PGD_attack(resnet50,res_data_Tensor,epsilon,iters,alpha)\n",
    "    \n",
    "with open(pgd_sample_path,'wb') as f:\n",
    "    content = pickle.dumps(pgd_sample)\n",
    "    f.write(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
